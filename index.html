<html>
<head>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@600&family=Poppins:wght@400;600&display=swap" rel="stylesheet">

  <style>
    body {
      font-family: 'Poppins', sans-serif;
      background-color: #ebebeb;
      color: #324058;
    }
    h1, h2, h3 {
      font-family: 'Poppins', serif;
      color: #4a9fb7;
      text-align: center;
    }
    .container {
      margin: 0 auto;
      padding: 60px 18%;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    p, li {
      line-height: 1.6;
    }
    img {
      max-width: 100%;
      border-radius: 10px;
      box-shadow: 0 3px 10px rgba(0,0,0,0.15);
    }
    figure {
      text-align: center;
      margin: 12px;
    }
    figcaption {
      font-size: 14px;
      color: #275673;
      margin-top: 6px;
    }
    table {
      border-collapse: collapse;
      margin: 0 auto;
    }
    th, td {
      padding: 8px;
      vertical-align: top;
    }
    pre, code {
      background: #d6dadf;
      padding: 12px;
      border-radius: 6px;
      font-family: monospace;
      font-size: 14px;
      overflow-x: auto;
      display: block;
    }
    img.zoomable { cursor: zoom-in; }
    #zoomModal {
      position: fixed;
      inset: 0;
      display: none;
      z-index: 9999;
      align-items: center;
      justify-content: center;
      background: rgba(18, 50, 62, 0.82);
    }
    #zoomModal.show { display: flex; }
    #zoomModal img {
      max-width: 90vw;
      max-height: 90vh;
      border-radius: 12px;
      box-shadow: 0 10px 30px rgba(0,0,0,.4);
      background: #111;
    }
    .bubble-link {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      text-align: center;
      padding: 8px 14px;
      border: 2px solid #4a9fb7;
      border-radius: 9999px;
      color: #4a9fb7;
      text-decoration: none;
      background: #f5fbfd;
    }
    .bubble-link:hover {
      background: #e7f4f8;
      color: #2f7e93;
      border-color: #2f7e93;
    }
    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(210px, 1fr));
      gap: 10px;
      margin: 20px 0;
      width: 100%;
    }
  </style>
</head>

<body>
<div class="container">
  <h1>CS180 Project 4: Neural Radiance Fields</h1>
  <div style="text-align:center;">By Inas Zulaikha Anwar</div>

  <!-- Cover render (lego or your own object) -->
  <div style="margin:20px 0; text-align:center;">
    <img src="images/2_4_lego_spin_gif.gif" alt="NeRF Lego Novel View" style="max-width:75%;">
  </div>

  <h2>Overview</h2>
  <p>
    In this project, I implemented a full Neural Radiance Field (NeRF) pipeline from scratch, starting from camera calibration and dataset creation all the way to novel-view synthesis. I first fit simple coordinate-based MLPs to 2D images to understand positional encoding and network capacity, then extended this framework to 3D by generating rays, sampling points, and performing volumetric rendering on the Lego multi-view dataset. Finally, I applied the same pipeline to my own captured scene, analyzing how data quality, camera coverage, and hyperparameters affect reconstruction quality and the realism of the resulting fly-throughs.
  </p>

  <!-- ================== PART 0 ================== -->
  <h2 id="part0">Part 0: Camera Calibration and Dataset Creation</h2>

  <h3>0.1 Calibrating My Camera</h3>
  <p>
    I first calibrated my phone camera using a grid of ArUco tags.
    For each calibration image, I detected the tags, picked the largest one,
    and used its four corners and the known physical tag size to collect 2D–3D correspondences.
    I then ran cv2.calibrateCamera to estimate the intrinsic matrix K and distortion coefficients,
    and saved them to intrinsics.npz, with an undistortion preview to sanity-check the result.
  </p>
  
  <table>
    <tr>
      <td>
        <figure>
          <img src="images/dbg_corners_0.jpg" alt="Detected ArUco corners in calibration image 0">
          <figcaption>detected ArUco corners in one calibration image</figcaption>
        </figure>
      </td>
      <td>
        <figure>
          <img src="images/dbg_corners_1.jpg" alt="Detected ArUco corners in calibration image 1">
          <figcaption>detected corners in a different viewpoint</figcaption>
        </figure>
      </td>
    </tr>
  </table>
  
  <figure>
    <img src="images/dbg_bad_reproj.jpg" alt="Example of bad reprojection during calibration">
    <figcaption>
      debug visualization of a bad reprojection case
    </figcaption>
  </figure>
  
  <h3>0.2 Capturing a 3D Object Scan</h3>
  <p>
    For my own NeRF scene, I photographed two separate scenes (and ended up completing the project with the Nailong scene).
    I photographed a Pokémon Legends: ZA game case placed next to a printed ArUco tag, and a Nailong toy.
    I took a small set of images from different viewpoints while trying to keep the tag visible,
    the object roughly centered, and the camera settings consistent with the calibration step (same device and focal length).
  </p>
  
  <h3>0.3 Estimating Camera Poses</h3>
  <p>
    Using the calibrated intrinsics, I estimated a camera pose for each object image.
    I first scanned all frames to find the ArUco tag ID that appears most often and treated that tag as the world origin.
    For each image containing that tag, I scaled K to image resolution, ran PnP variants, picked the pose with the lowest reprojection error, 
    converted from world-to-camera to camera-to-world, and saved all poses to 02_poses.npz.
  </p>
  <p>
    The screenshots below show the cloud of cameras surrounding the object and point more or
    less toward the same region in space, which matches how I captured the photos.
  </p>
  
  <table>
    <tr>
      <td>
        <figure>
          <img src="images/02_ss1.png" alt="Viser screenshot of camera frustums, view 1">
          <figcaption>viser visualization of pokemon scene.</figcaption>
        </figure>
      </td>
      <td>
        <figure>
          <img src="images/02_ss3.png" alt="Viser screenshot of camera frustums, view 2">
          <figcaption>viser visualization from a pokemon scene 2.</figcaption>
        </figure>
      </td>
    </tr>
    <tr>
      <td>
        <figure>
          <img src="images/02_new_render.png" alt="Viser screenshot of camera frustums, view 1">
          <figcaption>viser visualization of nailong scene.</figcaption>
        </figure>
      </td>
      <td>
        <figure>
          <img src="images/02_new_render_2.png" alt="Viser screenshot of camera frustums, view 2">
          <figcaption>viser visualization of nailong scene 2.</figcaption>
        </figure>
      </td>
    </tr>
  </table>
  
  <h3>0.4 Undistorting Images and Creating a Dataset</h3>
  <p>
    I turned my object photos and poses into a NeRF-ready dataset. Using the intrinsics from
    <i>intrinsics.npz</i>, I first scaled the calibration matrix \(K\) to each image’s resolution,
    undistorted the images, and resized them to a common target size given by the minimum height and width
    across the set.
  </p>
  <p>
    I then stacked the undistorted images into a single array, shuffled them with a fixed random seed,
    and split them into training and validation sets (about 90% train / 10% val) while keeping the
    corresponding camera-to-world poses from <i>02_poses.npz</i>. Finally, I saved everything into
    <i>my_data.npz</i> with image arrays, pose arrays, and a single scalar focal length, and wrote out
    <i>preview_train0.png</i> and <i>preview_val0.png</i> to quickly verify that the dataset looked correct.
  </p>
  
<!-- ================== PART 1 ================== -->
<h2 id="part1">Part 1: Neural Field for a 2D Image</h2>

<h3>1.1 Network Architecture and Positional Encoding</h3>
<p>
  In Part 1, I fit a coordinate-based neural network to individual 2D images. The model takes in
  normalized pixel coordinates \((u, v) \in [0,1]^2\) and predicts RGB values in \([0,1]^3\).
  I first apply sinusoidal positional encoding with maximum frequency \(L\), then feed the encoded
  coordinates into a small multilayer perceptron.
</p>
<ul>
  <li><b>Input:</b> 2D pixel coordinates normalized to \([0,1]\).</li>
  <li><b>Positional encoding:</b> for each scalar \(x\), I concatenate
      \(\sin(2^k \pi x)\) and \(\cos(2^k \pi x)\) for \(k = 0, \dots, L-1\), along with the original coordinates.</li>
  <li><b>MLP:</b> 3 fully-connected hidden layers with width 128 or 256 (depending on the experiment) and ReLU activations, followed by a final linear layer.</li>
  <li><b>Output layer:</b> linear layer followed by a sigmoid to map to RGB in \([0,1]\).</li>
  <li><b>Loss / optimization:</b> mean-squared error between predicted and ground-truth RGB, optimized with Adam
      (learning rate \(1\text{e-}2\)).</li>
  <li><b>Training:</b> at each step, I randomly sample a batch of pixels and train on their colors.</li>
</ul>
<p>
  Overall, this setup follows the structure suggested in the project spec: positional encoding on normalized coordinates, a small ReLU MLP, a sigmoid output to RGB in \([0,1]^3\), and an MSE loss optimized with Adam.
</p>

<h3>1.2 Training Progression on the Provided Image (Fox)</h3>
<p>
  For the provided “fox” image, I trained a higher-capacity model with 
  \(L = 10\) positional encoding frequencies and a hidden width of \(W = 256\).
  This larger model captures high-frequency texture more quickly and resolves edges
  more sharply than the smaller baseline. Below I show the reconstruction progression
  at several training iterations, arranged in rows of three for clearer visualization.
</p>

<table>
  <tr>
    <td>
      <figure>
        <img src="images/out_fox_L10_W256/step_0000.png" alt="Fox step 0">
        <figcaption>Step 0</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_fox_L10_W256/step_0050.png" alt="Fox step 50">
        <figcaption>Step 50</figcaption>
      </figure>
    </td>
  </tr>

  <tr>
    <td>
      <figure>
        <img src="images/out_fox_L10_W256/step_1000.png" alt="Fox step 1000">
        <figcaption>Step 1000</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_fox_L10_W256/step_3000.png" alt="Fox step 3000">
        <figcaption>Step 3000</figcaption>
      </figure>
    </td>
  </tr>

  <tr>
    <td colspan="3">
      <figure>
        <img src="images/fox.jpg" alt="Original fox image" style="max-width:70%;">
        <figcaption>Original image (ground truth)</figcaption>
      </figure>
    </td>
  </tr>
</table>

<h3>1.3 Training Progression on My Own Image (Ollie)</h3>
<p>
  I repeated the same experiment on my own image (“Ollie”) with \(L = 10\) and width \(W = 256\).
  The behavior is similar: the network first learns a blurry color field, then gradually sharpens
  object boundaries and small features as training progresses.
</p>

<table>
  <tr>
    <td>
      <figure>
        <img src="images/out_ollie_L10_W256/step_0000.png" alt="Ollie step 0">
        <figcaption>Step 0</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_ollie_L10_W256/step_0050.png" alt="Ollie step 50">
        <figcaption>Step 50</figcaption>
      </figure>
    </td>
  </tr>

  <tr>
    <td>
      <figure>
        <img src="images/out_ollie_L10_W256/step_1000.png" alt="Ollie step 1000">
        <figcaption>Step 1000</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_ollie_L10_W256/step_3000.png" alt="Ollie step 3000">
        <figcaption>Step 3000</figcaption>
      </figure>
    </td>
  </tr>

  <tr>
    <td colspan="3">
      <figure>
        <img src="images/ollie.jpg" alt="Original Ollie image" style="max-width:70%;">
        <figcaption>Original image (ground truth)</figcaption>
      </figure>
    </td>
  </tr>
</table>

<h3>1.4 Effect of Width and Positional Encoding Frequency</h3>
<p>
  To study how the architecture affects reconstruction quality, I trained four models on the fox image
  with different combinations of positional encoding frequency \(L\) and hidden width \(W\):
</p>
<ul>
  <li>\(L = 4, W = 128\)</li>
  <li>\(L = 4, W = 256\)</li>
  <li>\(L = 10, W = 128\)</li>
  <li>\(L = 10, W = 256\)</li>
</ul>
<p>
  Below are the final reconstructions (step 3000) for each setting.
  Higher \(L\) allows the network to represent finer details and sharper edges, while larger \(W\)
  increases capacity and tends to reduce artifacts and improve overall PSNR, at the cost of more computation.
</p>

<table>
  <tr>
    <th></th>
    <th>\(L = 4\)</th>
    <th>\(L = 10\)</th>
  </tr>
  <tr>
    <th>\(W = 128\)</th>
    <td>
      <figure>
        <img src="images/out_fox_L4_W128/step_3000.png" alt="Fox L4 W128 final reconstruction">
        <figcaption>L = 4, W = 128</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_fox_L10_W128/step_3000.png" alt="Fox L10 W128 final reconstruction">
        <figcaption>L = 10, W = 128</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <th>\(W = 256\)</th>
    <td>
      <figure>
        <img src="images/out_fox_L4_W256/step_3000.png" alt="Fox L4 W256 final reconstruction">
        <figcaption>L = 4, W = 256</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/out_fox_L10_W256/step_3000.png" alt="Fox L10 W256 final reconstruction">
        <figcaption>L = 10, W = 256</figcaption>
      </figure>
    </td>
  </tr>
</table>

<h3>1.5 PSNR Curve</h3>
<p>
  Finally, I plotted PSNR over training iterations for the fox image with \(L = 4\) and \(W = 128\).
  The curve rises quickly in the first few hundred iterations as the network captures low-frequency
  structure, then gradually plateaus as it refines high-frequency details and noise.
</p>
<figure>
  <img src="images/out_fox_L4_W128/psnr.png" alt="PSNR curve for fox image, L4 W128">
  <figcaption>PSNR vs. training iteration for the fox image (L = 4, W = 128).</figcaption>
</figure>

  
<h2 id="part2">Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

<h3>2.1 Ray Generation from Cameras</h3>
<p>
  In Part 2.1, I implemented the mapping from image pixels to 3D rays in world space.
  Given a camera pose matrix <em>c2w</em>, intrinsics <em>K</em>, and pixel coordinates,
  I first convert pixels into camera coordinates and then into world coordinates:
</p>
<ul>
  <li><em>transform</em>: takes points in camera space and applies the camera-to-world matrix
      to get points in world space.</li>
  <li><em>pixel_to_camera</em>: given a pixel <em>(u, v)</em> and a depth <em>s</em>, computes the
      camera-space point using <em>x<sub>c</sub> = s K<sup>-1</sup> [u, v, 1]<sup>T</sup></em>,
      with careful broadcasting so it works on batches of pixels.</li>
  <li><em>pixel_to_ray</em>: constructs a ray for each pixel by back-projecting to camera space,
      rotating into world space with the rotation from <em>c2w</em>, normalizing the direction,
      and using the camera center as the ray origin.</li>
</ul>
<p>
  This gives me a ray origin and unit direction for every pixel in every training image, which
  is the starting point for volumetric rendering in NeRF.
</p>

<h3>2.2 Sampling Points Along Rays</h3>
<p>
  In Part 2.2, I implemented stratified sampling along each ray. For each ray I linearly sample
  <em>n</em> depth values between a near and far plane (I used <em>near = 2</em> and <em>far = 6</em>),
  then optionally jitter each interval during training:
</p>
<ul>
  <li>Start with evenly spaced depth values <em>t</em> between near and far.</li>
  <li>Compute midpoints and sample a random point inside each bin when <em>perturb = True</em>.</li>
  <li>Convert these depth values into 3D sample points using <em>p = o + t d</em>, where <em>o</em>
      is the ray origin and <em>d</em> is the ray direction.</li>
</ul>
<p>
  The function returns both the sampled points and the corresponding depth values, which I later
  feed into the NeRF network and volume rendering step.
</p>

<h3>2.3 Ray Dataset and Visualization</h3>
<p>
  In Part 2.3, I wrapped everything in a small dataset class that can efficiently sample rays
  for training:
</p>
<ul>
  <li><em>RaysData</em> stores all training images, their camera poses, and a precomputed grid of
      pixel coordinates.</li>
  <li><em>sample_rays</em> can either pick a random image and sample random pixels from it
      (<em>per_image = True</em>) or sample pixels uniformly across all images.</li>
  <li>For each selected pixel, I call <em>pixel_to_ray</em> to get a ray origin and direction and
      also return the ground-truth RGB from the image.</li>
</ul>

<figure>
  <img src="images/2_3_rays.png" alt="Visualization of cameras, rays, and samples using viser">
  <figcaption>
    Cameras, sampled rays, and sample points for the Lego training set visualized with viser.
  </figcaption>
</figure>

<h3>2.4 Training a NeRF on the Lego Dataset</h3>

<p>
In this part, I trained a NeRF model on the Lego multi-view dataset using the
ray sampler and MLP from the previous parts. For each iteration, I sample a
batch of rays, query the network at stratified points along each ray, and use
volumetric rendering to composite the predicted colors. The training objective
is the MSE loss between the rendered RGB values and the ground-truth pixel
colors, and I track performance using PSNR on a held-out validation set.
</p>

<figure>
  <img src="images/2_4_rays_samples.png"
       alt="Visualization of rays and samples"
       style="max-width: 70%;">
  <figcaption>
    visualization of camera rays and sampled 3D points
  </figcaption>
</figure>

<figure>
  <img src="images/2_4_psnrcurve.png"
       alt="PSNR curve on the validation set"
       style="max-width: 70%;">
  <figcaption>
    PSNR on the validation set over training iterations
  </figcaption>
</figure>

<h4>Training Progression</h4>

<table style="border-collapse: collapse; text-align: center;">
  <tr>
    <th style="padding: 4px;">Step 150</th>
    <th style="padding: 4px;">Step 500</th>
    <th style="padding: 4px;">Step 750</th>
  </tr>
  <tr>
    <td style="padding: 4px;">
      <img src="images/2_4_step150.png"
           alt="Rendered Lego at step 150"
           style="max-width: 110px; display: block; margin: 0 auto;">
    </td>
    <td style="padding: 4px;">
      <img src="images/2_4_step500.png"
           alt="Rendered Lego at step 500"
           style="max-width: 110px; display: block; margin: 0 auto;">
    </td>
    <td style="padding: 4px;">
      <img src="images/2_4_step750.png"
           alt="Rendered Lego at step 750"
           style="max-width: 110px; display: block; margin: 0 auto;">
    </td>
  </tr>
  <tr>
  </tr>
  <tr>
    <th style="padding: 4px;">Step 1500</th>
    <th style="padding: 4px;">Step 2000</th>
    <th style="padding: 4px;"></th>
  </tr>
  <tr>
    <td style="padding: 4px;">
      <img src="images/2_4_step1500.png"
           alt="Rendered Lego at step 1500"
           style="max-width: 110px; display: block; margin: 0 auto;">
    </td>
    <td style="padding: 4px;">
      <img src="images/2_4_step2000.png"
           alt="Rendered Lego at step 2000"
           style="max-width: 110px; display: block; margin: 0 auto;">
    </td>
    <td></td>
  </tr>
  <tr>
    <td colspan="3" style="padding: 4px; font-size: 0.9em;">
      As training progresses, the shape, edges, and colors become much sharper,
      and the Lego figure is clearly recognizable with fewer artifacts.
    </td>
  </tr>
</table>

<figure>
  <img src="images/2_4_lego_spin_gif.gif"
       alt="Spherical Lego NeRF rendering"
       style="max-width: 60%;">
  <figcaption>
    Spherical rendering video of the Lego
  </figcaption>
</figure>

  
  <h3>2.5 Volume Rendering</h3>
  <p>
    I implemented the discrete volume rendering equation in <i>volrend</i>.
    For each ray with samples indexed by \(i\), I compute the per-sample opacity
    \[
      \alpha_i = 1 - \exp(-\sigma_i \Delta t_i).
    \]
    I then compute the accumulated transmittance using a cumulative product:
    \[
      T_i = \prod_{j &lt; i} (1 - \alpha_j).
    \]
    I formed <i>one_minus_a = 1 - alpha</i>, prepended a column of ones, and called <i>torch.cumprod</i>,
    then drop the last column to align \(T\) with the samples. The weights are
    <i>weights = alpha * T</i>, and the final color is the weighted sum
  </p>
  <pre><code>rgb_map = (weights[..., None] * rgbs).sum(dim=1)</code></pre>
  <p>
    Then, I computed the accumulated opacity <i>acc = weights.sum(dim=1, keepdim=True)</i> and optionally composite onto a white
    background as <i>rgb_map + (1 - acc)</i> when <i>white_bg=True</i>. I verified correctness using the staff’s test snippet
    with random <i>sigmas</i> and <i>rgbs</i>; my <i>volrend</i> output matches the reference tensor within <i>1e-4</i> tolerance.
  </p>
  <h2>Part 2.6: Training with My Own Data</h2>

<p>
  I struggled quite a bit with this one, ending up with an imperfect render.
  I trained a NeRF on my own captured dataset of a small yellow
  Nailong figurine placed on a table, and realized that I made a mistake with the color channels.
  However, I moved along and followed the full pipeline used earlier: ArUco-based camera calibration, pose estimation for each image, undistortion
  and resizing, and finally packaging everything into a NeRF-compatible dataset.
</p>

<h3>Training Setup</h3>
<p>
  I trained the NeRF for about 2000 iterations using the following settings:
</p>
<ul>
  <li><strong>Positional encodings:</strong> L<sub>x</sub> = 10, L<sub>d</sub> = 4</li>
  <li><strong>Network width:</strong> 128–256 (depending on GPU memory)</li>
  <li><strong>Batch size:</strong> 4096 rays</li>
  <li><strong>Samples per ray:</strong> 32</li>
  <li><strong>Near / far planes:</strong> 0.1 and 0.5</li>
  <li><strong>Learning rate:</strong> 5e-4</li>
</ul>

<p>
  During training, the model converged in terms of MSE loss, and the validation
  PSNR steadily improved from roughly 14&nbsp;dB at the start to about
  18&ndash;19&nbsp;dB by 2000 iterations.
</p>

<figure>
  <img src="images/2_6_train_loss_curve.png" alt="Training loss vs. iteration plot for my own dataset." style="max-width: 100%; height: auto;">
  <figcaption>Training loss vs. iteration for my own dataset</figcaption>
</figure>

<figure>
  <img src="images/2_6_val_psnr_curve.png" alt="Validation PSNR vs. iteration plot for my own dataset." style="max-width: 100%; height: auto;">
  <figcaption>Validation PSNR vs. iteration for my own dataset</figcaption>
</figure>

<h3>Intermediate Renders and Final GIF</h3>
<p>
  I generated intermediate validation renders during training, but they definitely could use work.
  The model captured some coarse structure and overall color tone (that was in the dataset), but the reconstructions
  remained very blurry and streaked. This is consistent with the moderate PSNR
  values and suggests that the network is learning some aspects of the scene,
  but not enough detail to clearly reconstruct the object.
</p>

<figure>
  <img src="images/2_6_gif.gif" alt="GIF of my NeRF rendering novel views of the Nailong scene." style="max-width: 100%; height: auto;">
  <figcaption>GIF of the NeRF rendering novel views of my Nailong scene</figcaption>
</figure>

<table style="width:100%; text-align:center;">
  <tr>
    <td>
      <figure>
        <!-- e.g. images/nailong_val_step_00200.png -->
        <img src="images/2_6_1.png"
             alt="Validation render at step 200"
             style="max-width:100%; height:auto;">
        <figcaption>
          Step 200: still very blurry
        </figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <!-- e.g. images/nailong_val_step_00800.png -->
        <img src="images/2_6_800.png"
             alt="Validation render at step 800"
             style="max-width:100%; height:auto;">
        <figcaption>
          Step 800: some diagonal structure appears, but the object shape is
          still hard to make out.
        </figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td>
      <figure>
        <!-- e.g. images/nailong_val_step_01400.png -->
        <img src="images/2_6_1400.png"
             alt="Validation render at step 1400"
             style="max-width:100%; height:auto;">
        <figcaption>
          Step 1400: scene layout becomes more consistent, though
          Nailong is still very smeared.
        </figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <!-- e.g. images/nailong_val_step_02000.png -->
        <img src="images/2_6_2000.png"
             alt="Validation render at step 2000"
             style="max-width:100%; height:auto;">
        <figcaption>
          Step 2000: overall structure is somewhat recognizable but fine
          details never fully appear, reflecting the limited data quality.
        </figcaption>
      </figure>
    </td>
  </tr>
</table>

<h3>Problems Faced</h3>
<p>
  Even though the training curves look reasonable, the final render is still
  quite abstract. There could be several reasons for this:
</p>
<ul>
  <li>
    <strong>Limited and challenging data:</strong>
    the object is small, close to the camera, and placed on a reflective table
    with a visually busy background. It may be difficult NeRF to separate
    the toy from the environment.
  </li>
  <li>
    <strong>Pose quality and camera coverage:</strong>
    although I estimated poses with ArUco tags, the cameras are clustered fairly
    tightly around the object, so there is limited space.
  </li>
  <li>
    <strong>Downsampled resolution:</strong>
    to keep training time manageable, I resized the images to a smaller
    resolution. This speeds up training but removes fine detail that the network
    could otherwise learn.
  </li>
</ul>

<h3>Hyperparameter and Code Experiments</h3>
<p>
  To try to improve the reconstruction quality, I experimented with several
  changes:
</p>
<ul>
  <li>varying near/far planes (for example, [0.05, 0.3] and [0.2, 0.6]) to better match the true scene depth.</li>
  <li>increasing the MLP width to 256 for a higher-capacity network (at the cost of slower training).</li>
  <li>training for longer (up to several thousand iterations) when time and compute allowed.</li>
  <li>filtering out images with unreliable tag detections or partial occlusions of the object.</li>
</ul>

<p>
  These tweaks helped the loss and PSNR curves, but the final render stayed
  blurry, which suggests that the main bottleneck is the dataset quality and
  capture setup rather than just the network architecture or learning rate.
</p>

<h3>Takeaways</h3>
<p>
  Even though my NeRF of the Nailong toy did not reconstruct a crisp 3D model, I'm grateful for this project and
  the process was still very valuable. I implemented the full pipeline
  end-to-end, from camera calibration and pose estimation to dataset
  creation, NeRF training, and novel-view rendering. The results highlight how
  sensitive NeRFs are to camera coverage, pose accuracy, and clean, consistent
  input images, and show the practical gap between controlled datasets and quickly captured real-world data.
</p>

</div>

<div id="zoomModal" aria-hidden="true">
  <img alt="">
</div>

<script>
  (function () {
    const modal = document.getElementById('zoomModal');
    const modalImg = modal.querySelector('img');

    document.querySelectorAll('img:not([data-nozoom])').forEach(img => {
      img.classList.add('zoomable');
      img.addEventListener('click', () => {
        modalImg.src = img.src;
        modal.classList.add('show');
        modal.setAttribute('aria-hidden', 'false');
      });
    });

    modal.addEventListener('click', () => {
      modal.classList.remove('show');
      modal.setAttribute('aria-hidden', 'true');
      modalImg.src = '';
    });
    document.addEventListener('keydown', e => {
      if (e.key === 'Escape' && modal.classList.contains('show')) modal.click();
    });
  })();
</script>

</body>
</html>
